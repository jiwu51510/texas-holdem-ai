{
  // ==================== 基础训练参数 ====================
  
  // 学习率：控制神经网络权重更新的步长
  // 建议范围：0.0001 - 0.01，较小的值训练更稳定但更慢
  "learning_rate": 0.001,
  
  // 批次大小：每次训练使用的样本数量
  // Deep CFR 通常使用较大批次（1024-4096），更大的批次需要更多内存
  "batch_size": 2048,
  
  // 训练回合数：总共进行的自博弈对局数
  // 建议：快速测试用1000-5000，正式训练用50000-100000+
  "num_episodes": 10000,
  
  // 折扣因子：未来奖励的折扣系数
  // CFR算法通常设为1.0（不折扣），强化学习常用0.99
  "discount_factor": 1.0,
  
  // ==================== 神经网络架构 ====================
  
  // 网络隐藏层维度：定义神经网络的结构
  // [512, 256, 128] 表示3个隐藏层，分别有512、256、128个神经元
  // 更大的网络容量更强但训练更慢
  "network_architecture": [512, 256, 128],
  
  // ==================== 检查点与并行 ====================
  
  // 检查点保存间隔：每隔多少回合保存一次模型
  // 建议设置为 num_episodes 的 5%-10%
  "checkpoint_interval": 1000,
  
  // 并行环境数量：同时运行的游戏环境数
  // 增加可以加速训练，但会占用更多CPU/内存
  "num_parallel_envs": 1,
  
  // ==================== 游戏规则参数 ====================
  
  // 初始筹码：每局游戏开始时玩家的筹码数
  // 通常设为大盲注的100倍（100BB）
  "initial_stack": 1000,
  
  // 小盲注：强制下注的较小金额
  "small_blind": 5,
  
  // 大盲注：强制下注的较大金额，通常是小盲注的2倍
  "big_blind": 10,
  
  // ==================== Deep CFR 特有参数 ====================
  
  // 遗憾缓冲区大小：存储遗憾值样本的蓄水池容量
  // 更大的缓冲区可以保留更多历史样本，但占用更多内存
  // 建议：1000000 - 5000000
  "regret_buffer_size": 2000000,
  
  // 策略缓冲区大小：存储策略样本的蓄水池容量
  // 通常与遗憾缓冲区大小相同
  "strategy_buffer_size": 2000000,
  
  // CFR迭代次数：每次网络更新前进行的CFR遍历次数
  // 更多迭代产生更准确的遗憾估计，但训练更慢
  // 建议：500 - 2000
  "cfr_iterations_per_update": 1000,
  
  // 网络训练步数：每次更新时训练神经网络的步数
  // 更多步数使网络更好地拟合缓冲区数据
  // 建议：2000 - 8000
  "network_train_steps": 4000
}
