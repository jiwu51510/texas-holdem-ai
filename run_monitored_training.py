#!/usr/bin/env python3
"""ç›‘æ§è®­ç»ƒè„šæœ¬ã€‚

ä½¿ç”¨æœ€ä¼˜é…ç½®è¿›è¡Œè®­ç»ƒï¼Œå¹¶å®æ—¶ç›‘æ§å„é¡¹æŒ‡æ ‡ã€‚
å¦‚æœæŒ‡æ ‡ä¸ç¬¦åˆé¢„æœŸï¼Œè‡ªåŠ¨åœæ­¢å¹¶è°ƒæ•´å‚æ•°ã€‚
"""

import argparse
import json
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import numpy as np

import torch

from models.core import TrainingConfig, Card
from train_river_only import RiverOnlyTrainer, parse_board, board_to_str


@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡ã€‚"""
    iteration: int = 0
    regret_loss: float = 0.0
    policy_loss: float = 0.0
    regret_grad_norm: float = 0.0
    policy_grad_norm: float = 0.0
    entropy: float = 0.0
    is_oscillating: bool = False
    kl_divergence: float = 0.0
    p0_win_rate: float = 0.0
    avg_utility_p0: float = 0.0
    training_time: float = 0.0


@dataclass
class TrainingThresholds:
    """è®­ç»ƒé˜ˆå€¼é…ç½®ã€‚"""
    # æŸå¤±é˜ˆå€¼
    max_regret_loss: float = 5.0  # é—æ†¾æŸå¤±è¶…è¿‡æ­¤å€¼åˆ™è­¦å‘Š
    max_policy_loss: float = 3.0  # ç­–ç•¥æŸå¤±è¶…è¿‡æ­¤å€¼åˆ™è­¦å‘Š
    
    # æ¢¯åº¦é˜ˆå€¼
    max_grad_norm: float = 10.0  # æ¢¯åº¦èŒƒæ•°è¶…è¿‡æ­¤å€¼åˆ™è­¦å‘Š
    
    # æ”¶æ•›é˜ˆå€¼
    target_regret_loss: float = 0.5  # ç›®æ ‡é—æ†¾æŸå¤±
    target_policy_loss: float = 1.0  # ç›®æ ‡ç­–ç•¥æŸå¤±
    
    # éœ‡è¡æ£€æµ‹ - æ”¾å®½å®¹å¿åº¦ï¼Œå› ä¸ºCFRè®­ç»ƒä¸­éœ‡è¡æ˜¯æ­£å¸¸çš„
    oscillation_patience: int = 20  # è¿ç»­éœ‡è¡æ¬¡æ•°è¶…è¿‡æ­¤å€¼åˆ™åœæ­¢
    stop_on_oscillation: bool = False  # æ˜¯å¦å› éœ‡è¡è€Œåœæ­¢
    
    # æŸå¤±å¢é•¿æ£€æµ‹
    loss_increase_patience: int = 5  # è¿ç»­æŸå¤±å¢é•¿æ¬¡æ•°è¶…è¿‡æ­¤å€¼åˆ™è­¦å‘Š


class MonitoredTrainer:
    """ç›‘æ§è®­ç»ƒå™¨ã€‚"""
    
    def __init__(
        self,
        config_path: str,
        fixed_board: Optional[List[Card]] = None,
        thresholds: Optional[TrainingThresholds] = None
    ):
        """åˆå§‹åŒ–ç›‘æ§è®­ç»ƒå™¨ã€‚
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            fixed_board: å›ºå®šå…¬å…±ç‰Œ
            thresholds: è®­ç»ƒé˜ˆå€¼é…ç½®
        """
        self.config_path = config_path
        self.fixed_board = fixed_board
        self.thresholds = thresholds or TrainingThresholds()
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config_dict = json.load(f)
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        self.training_config = self._create_training_config()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = RiverOnlyTrainer(self.training_config, fixed_board=fixed_board)
        
        # åº”ç”¨æ”¶æ•›æ§åˆ¶å‚æ•°
        self._apply_convergence_params()
        
        # è®­ç»ƒå†å²
        self.metrics_history: List[TrainingMetrics] = []
        
        # çŠ¶æ€è·Ÿè¸ª
        self.consecutive_oscillations = 0
        self.consecutive_loss_increases = 0
        self.last_regret_loss = float('inf')
        self.best_regret_loss = float('inf')
        self.should_stop = False
        self.stop_reason = ""
    
    def _create_training_config(self) -> TrainingConfig:
        """ä»é…ç½®å­—å…¸åˆ›å»ºè®­ç»ƒé…ç½®ã€‚"""
        return TrainingConfig(
            learning_rate=self.config_dict.get('learning_rate', 0.0001),
            batch_size=self.config_dict.get('batch_size', 256),
            network_architecture=self.config_dict.get('network_architecture', [512, 256, 128]),
            cfr_iterations_per_update=self.config_dict.get('cfr_iterations_per_update', 1000),
            network_train_steps=self.config_dict.get('network_train_steps', 2000),
            regret_buffer_size=self.config_dict.get('regret_buffer_size', 300000),
            strategy_buffer_size=self.config_dict.get('strategy_buffer_size', 300000),
            initial_stack=self.config_dict.get('initial_stack', 1000),
            small_blind=self.config_dict.get('small_blind', 5),
            big_blind=self.config_dict.get('big_blind', 10),
            max_raises_per_street=self.config_dict.get('max_raises_per_street', 4),
        )
    
    def _apply_convergence_params(self):
        """åº”ç”¨æ”¶æ•›æ§åˆ¶å‚æ•°åˆ°è®­ç»ƒå™¨ã€‚"""
        # é—æ†¾å€¼å¤„ç†å™¨é…ç½®
        regret_config = self.config_dict.get('regret_processor', {})
        self.trainer.regret_processor.config.use_positive_truncation = regret_config.get('use_positive_truncation', True)
        self.trainer.regret_processor.config.decay_factor = regret_config.get('decay_factor', 0.9)
        self.trainer.regret_processor.config.clip_threshold = regret_config.get('clip_threshold', 15.0)
        
        # ç½‘ç»œè®­ç»ƒå™¨é…ç½®
        network_config = self.config_dict.get('network_trainer', {})
        self.trainer.network_trainer.config.use_huber_loss = network_config.get('use_huber_loss', True)
        self.trainer.network_trainer.config.huber_delta = network_config.get('huber_delta', 0.2)
        self.trainer.network_trainer.config.use_ema = network_config.get('use_ema', True)
        self.trainer.network_trainer.config.ema_decay = network_config.get('ema_decay', 0.999)
        self.trainer.network_trainer.config.gradient_clip_norm = network_config.get('gradient_clip_norm', 0.1)
        
        # æ”¶æ•›ç›‘æ§å™¨é…ç½®
        monitor_config = self.config_dict.get('convergence_monitor', {})
        self.trainer.convergence_monitor.config.entropy_window = monitor_config.get('entropy_window', 100)
        self.trainer.convergence_monitor.config.oscillation_threshold = monitor_config.get('oscillation_threshold', 0.1)
        self.trainer.convergence_monitor.config.kl_warning_threshold = monitor_config.get('kl_warning_threshold', 0.5)
        self.trainer.convergence_monitor.config.monitor_interval = monitor_config.get('monitor_interval', 500)
    
    def _check_metrics(self, metrics: TrainingMetrics) -> List[str]:
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
        
        Returns:
            è­¦å‘Šæ¶ˆæ¯åˆ—è¡¨
        """
        warnings = []
        
        # æ£€æŸ¥æŸå¤±
        if metrics.regret_loss > self.thresholds.max_regret_loss:
            warnings.append(f"âš ï¸ é—æ†¾æŸå¤±è¿‡é«˜: {metrics.regret_loss:.4f} > {self.thresholds.max_regret_loss}")
        
        if metrics.policy_loss > self.thresholds.max_policy_loss:
            warnings.append(f"âš ï¸ ç­–ç•¥æŸå¤±è¿‡é«˜: {metrics.policy_loss:.4f} > {self.thresholds.max_policy_loss}")
        
        # æ£€æŸ¥æ¢¯åº¦
        if metrics.regret_grad_norm > self.thresholds.max_grad_norm:
            warnings.append(f"âš ï¸ é—æ†¾æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {metrics.regret_grad_norm:.4f} > {self.thresholds.max_grad_norm}")
        
        if metrics.policy_grad_norm > self.thresholds.max_grad_norm:
            warnings.append(f"âš ï¸ ç­–ç•¥æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {metrics.policy_grad_norm:.4f} > {self.thresholds.max_grad_norm}")
        
        # æ£€æŸ¥éœ‡è¡
        if metrics.is_oscillating:
            self.consecutive_oscillations += 1
            if self.consecutive_oscillations >= self.thresholds.oscillation_patience:
                warnings.append(f"âš ï¸ è¿ç»­éœ‡è¡ {self.consecutive_oscillations} æ¬¡")
                if self.thresholds.stop_on_oscillation:
                    warnings.append(f"ğŸ›‘ å› éœ‡è¡åœæ­¢è®­ç»ƒ")
                    self.should_stop = True
                    self.stop_reason = "è¿ç»­éœ‡è¡è¿‡å¤š"
        else:
            self.consecutive_oscillations = 0
        
        # æ£€æŸ¥æŸå¤±å¢é•¿
        if metrics.regret_loss > self.last_regret_loss * 1.1:  # æŸå¤±å¢é•¿è¶…è¿‡10%
            self.consecutive_loss_increases += 1
            if self.consecutive_loss_increases >= self.thresholds.loss_increase_patience:
                warnings.append(f"âš ï¸ è¿ç»­æŸå¤±å¢é•¿ {self.consecutive_loss_increases} æ¬¡")
        else:
            self.consecutive_loss_increases = 0
        
        self.last_regret_loss = metrics.regret_loss
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if metrics.regret_loss < self.best_regret_loss:
            self.best_regret_loss = metrics.regret_loss
        
        return warnings
    
    def _print_progress(self, metrics: TrainingMetrics, warnings: List[str]):
        """æ‰“å°è®­ç»ƒè¿›åº¦ã€‚"""
        print(f"\n{'='*70}")
        print(f"è¿­ä»£ {metrics.iteration}")
        print(f"{'='*70}")
        print(f"  é—æ†¾æŸå¤±: {metrics.regret_loss:.6f} (æœ€ä½³: {self.best_regret_loss:.6f})")
        print(f"  ç­–ç•¥æŸå¤±: {metrics.policy_loss:.6f}")
        print(f"  é—æ†¾æ¢¯åº¦èŒƒæ•°: {metrics.regret_grad_norm:.4f}")
        print(f"  ç­–ç•¥æ¢¯åº¦èŒƒæ•°: {metrics.policy_grad_norm:.4f}")
        print(f"  ç†µ: {metrics.entropy:.4f}")
        print(f"  éœ‡è¡: {'æ˜¯' if metrics.is_oscillating else 'å¦'}")
        print(f"  P0èƒœç‡: {metrics.p0_win_rate:.2%}")
        print(f"  P0å¹³å‡æ”¶ç›Š: {metrics.avg_utility_p0:.2f}")
        print(f"  è®­ç»ƒæ—¶é—´: {metrics.training_time:.1f}ç§’")
        
        if warnings:
            print(f"\nè­¦å‘Š:")
            for warning in warnings:
                print(f"  {warning}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if metrics.regret_loss <= self.thresholds.target_regret_loss:
            print(f"\nâœ… é—æ†¾æŸå¤±å·²è¾¾åˆ°ç›®æ ‡ ({metrics.regret_loss:.4f} <= {self.thresholds.target_regret_loss})")
    
    def train(
        self,
        total_iterations: int,
        cfr_per_update: int,
        eval_interval: int = 5000,
        checkpoint_interval: int = 10000,
        checkpoint_dir: str = "checkpoints/monitored"
    ) -> Dict[str, Any]:
        """æ‰§è¡Œç›‘æ§è®­ç»ƒã€‚
        
        Args:
            total_iterations: æ€»è¿­ä»£æ¬¡æ•°
            cfr_per_update: æ¯æ¬¡ç½‘ç»œæ›´æ–°çš„CFRè¿­ä»£æ¬¡æ•°
            eval_interval: è¯„ä¼°é—´éš”
            checkpoint_interval: æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
            checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœæ‘˜è¦
        """
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        print(f"\n{'#'*70}")
        print(f"å¼€å§‹ç›‘æ§è®­ç»ƒ")
        print(f"{'#'*70}")
        print(f"é…ç½®æ–‡ä»¶: {self.config_path}")
        print(f"å…¬å…±ç‰Œæ¨¡å¼: {self.trainer.get_board_mode()}")
        print(f"æ€»è¿­ä»£æ¬¡æ•°: {total_iterations}")
        print(f"CFRè¿­ä»£/æ›´æ–°: {cfr_per_update}")
        print(f"è¯„ä¼°é—´éš”: {eval_interval}")
        print(f"æ£€æŸ¥ç‚¹é—´éš”: {checkpoint_interval}")
        print(f"{'#'*70}\n")
        
        start_time = time.time()
        update_count = 0
        
        while self.trainer.iteration < total_iterations and not self.should_stop:
            update_start = time.time()
            
            # CFR è¿­ä»£
            for _ in range(cfr_per_update):
                self.trainer.run_cfr_iteration(verbose=False)
            
            # è®­ç»ƒç½‘ç»œ
            train_results = self.trainer.train_networks(verbose=False)
            
            update_count += 1
            
            # æ”¶é›†æŒ‡æ ‡
            convergence_report = train_results.get('convergence_report', {})
            latest_metrics = convergence_report.get('latest_metrics', {})
            
            metrics = TrainingMetrics(
                iteration=self.trainer.iteration,
                regret_loss=train_results.get('regret_loss', 0.0),
                policy_loss=train_results.get('policy_loss', 0.0),
                regret_grad_norm=train_results.get('regret_grad_norm', 0.0),
                policy_grad_norm=train_results.get('policy_grad_norm', 0.0),
                entropy=latest_metrics.get('avg_entropy', 0.0),
                is_oscillating=latest_metrics.get('is_oscillating', False),
                kl_divergence=latest_metrics.get('kl_divergence', 0.0),
                training_time=time.time() - update_start
            )
            
            # å®šæœŸè¯„ä¼°
            if self.trainer.iteration % eval_interval == 0:
                eval_results = self.trainer.evaluate_strategy(num_hands=500)
                metrics.p0_win_rate = eval_results['p0_win_rate']
                metrics.avg_utility_p0 = eval_results['avg_utility_p0']
            
            self.metrics_history.append(metrics)
            
            # æ£€æŸ¥æŒ‡æ ‡
            warnings = self._check_metrics(metrics)
            
            # æ‰“å°è¿›åº¦ï¼ˆæ¯æ¬¡æ›´æ–°éƒ½æ‰“å°ï¼‰
            self._print_progress(metrics, warnings)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.trainer.iteration % checkpoint_interval == 0:
                checkpoint_path = os.path.join(
                    checkpoint_dir,
                    f"checkpoint_{self.trainer.iteration}.pt"
                )
                self.trainer.save_checkpoint(checkpoint_path)
        
        # è®­ç»ƒç»“æŸ
        total_time = time.time() - start_time
        
        # æœ€ç»ˆè¯„ä¼°
        print(f"\n{'#'*70}")
        print(f"è®­ç»ƒå®Œæˆ")
        print(f"{'#'*70}")
        
        final_eval = self.trainer.evaluate_strategy(num_hands=1000)
        
        result = {
            'total_iterations': self.trainer.iteration,
            'total_time_seconds': total_time,
            'final_regret_loss': self.metrics_history[-1].regret_loss if self.metrics_history else 0.0,
            'final_policy_loss': self.metrics_history[-1].policy_loss if self.metrics_history else 0.0,
            'best_regret_loss': self.best_regret_loss,
            'final_p0_win_rate': final_eval['p0_win_rate'],
            'final_avg_utility_p0': final_eval['avg_utility_p0'],
            'stopped_early': self.should_stop,
            'stop_reason': self.stop_reason,
        }
        
        print(f"æ€»è¿­ä»£æ¬¡æ•°: {result['total_iterations']}")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {result['total_time_seconds']:.1f}ç§’")
        print(f"æœ€ç»ˆé—æ†¾æŸå¤±: {result['final_regret_loss']:.6f}")
        print(f"æœ€ä½³é—æ†¾æŸå¤±: {result['best_regret_loss']:.6f}")
        print(f"æœ€ç»ˆP0èƒœç‡: {result['final_p0_win_rate']:.2%}")
        print(f"æœ€ç»ˆP0å¹³å‡æ”¶ç›Š: {result['final_avg_utility_p0']:.2f}")
        
        if self.should_stop:
            print(f"\nâš ï¸ è®­ç»ƒæå‰åœæ­¢: {self.stop_reason}")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        final_checkpoint_path = os.path.join(checkpoint_dir, "checkpoint_final.pt")
        self.trainer.save_checkpoint(final_checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        report_path = os.path.join(checkpoint_dir, "training_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nè®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return result


def main():
    """ä¸»å‡½æ•°ã€‚"""
    parser = argparse.ArgumentParser(description='ç›‘æ§è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='configs/river_optimized_config.json',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--board', type=str, default="AhKsQdJc2h",
                        help='å›ºå®šå…¬å…±ç‰Œ')
    parser.add_argument('--iterations', type=int, default=50000,
                        help='æ€»è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--cfr-per-update', type=int, default=1000,
                        help='æ¯æ¬¡ç½‘ç»œæ›´æ–°çš„CFRè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--eval-interval', type=int, default=5000,
                        help='è¯„ä¼°é—´éš”')
    parser.add_argument('--checkpoint-interval', type=int, default=10000,
                        help='æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”')
    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints/monitored',
                        help='æ£€æŸ¥ç‚¹ç›®å½•')
    
    args = parser.parse_args()
    
    # è§£æå…¬å…±ç‰Œ
    fixed_board = None
    if args.board:
        try:
            fixed_board = parse_board(args.board)
            print(f"ä½¿ç”¨å›ºå®šå…¬å…±ç‰Œ: {board_to_str(fixed_board)}")
        except ValueError as e:
            print(f"è­¦å‘Š: æ— æ³•è§£æå…¬å…±ç‰Œ '{args.board}': {e}")
            print("å°†ä½¿ç”¨éšæœºå…¬å…±ç‰Œ")
    
    # åˆ›å»ºç›‘æ§è®­ç»ƒå™¨
    trainer = MonitoredTrainer(
        config_path=args.config,
        fixed_board=fixed_board
    )
    
    # å¼€å§‹è®­ç»ƒ
    result = trainer.train(
        total_iterations=args.iterations,
        cfr_per_update=args.cfr_per_update,
        eval_interval=args.eval_interval,
        checkpoint_interval=args.checkpoint_interval,
        checkpoint_dir=args.checkpoint_dir
    )
    
    return result


if __name__ == "__main__":
    main()
